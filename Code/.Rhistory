}
sigma[, , k] <- temp/sum.p[k] + 0.01 * diag(6)
}
}
sigma
for(i in 1:30){
# E-step
for (k in 1:K){
d[, k] <- pi[k]*dmvnorm(X, mu[k, ], sigma[, , k])
}
total.d <- rowSums(d)
for (k in 1:K){
p[, k] <- pi[k]*dmvnorm(X, mu[k, ], sigma[, , k])/total.d
}
# M-step
pi <- colSums(p)/N
sum.p <- colSums(p)
for (k in 1:K){
mu[k, ] <- colSums(p[, k]*X)/sum.p[k]
}
for (k in 1:K){
temp <- 0
for(j in 1:N){
temp <- temp + p[j, k]*(X[j, ] - mu[k, ])%*%t(X[j, ] - mu[k, ])
}
sigma[, , k] <- temp/sum.p[k] + 1.0 * diag(6)
}
}
sigma
for(i in 1:40){
# E-step
for (k in 1:K){
d[, k] <- pi[k]*dmvnorm(X, mu[k, ], sigma[, , k])
}
total.d <- rowSums(d)
for (k in 1:K){
p[, k] <- pi[k]*dmvnorm(X, mu[k, ], sigma[, , k])/total.d
}
# M-step
pi <- colSums(p)/N
sum.p <- colSums(p)
for (k in 1:K){
mu[k, ] <- colSums(p[, k]*X)/sum.p[k]
}
for (k in 1:K){
temp <- 0
for(j in 1:N){
temp <- temp + p[j, k]*(X[j, ] - mu[k, ])%*%t(X[j, ] - mu[k, ])
}
sigma[, , k] <- temp/sum.p[k] + 1.0 * diag(6)
}
}
mu
p
K
p <- matrix(NA, N, K)
p
X
mu[1,]
pi[1]*dmvnorm(X, mu[1, ], sigma[, , 1])
summary(X)
(centered.x <- scale(x,center = FALSE, scale = TRUE))
centered.scaled.x <- scale(x)
X = data.matrix(centered.scaled.x)
summary(X)
N <- nrow(X)
K = 4
pi <- rep(1/K, K)
sigma <- array(diag(6), c(6, 6, K))
mu <- cbind(runif(K, min(X[, 1]), max(X[, 1])), runif(K, min(X[, 2]), max(X[, 2])),
runif(K, min(X[, 3]), max(X[, 3])),runif(K, min(X[, 4]), max(X[, 4])),
runif(K, min(X[, 5]), max(X[, 5])),runif(K, min(X[, 6]), max(X[, 6])))
d <- matrix(NA, N, K)
p <- matrix(NA, N, K)
for(i in 1:40){
# E-step
for (k in 1:K){
d[, k] <- pi[k]*dmvnorm(X, mu[k, ], sigma[, , k])
}
total.d <- rowSums(d)
for (k in 1:K){
p[, k] <- pi[k]*dmvnorm(X, mu[k, ], sigma[, , k])/total.d
}
# M-step
pi <- colSums(p)/N
sum.p <- colSums(p)
for (k in 1:K){
mu[k, ] <- colSums(p[, k]*X)/sum.p[k]
}
for (k in 1:K){
temp <- 0
for(j in 1:N){
temp <- temp + p[j, k]*(X[j, ] - mu[k, ])%*%t(X[j, ] - mu[k, ])
}
sigma[, , k] <- temp/sum.p[k] + 1.0 * diag(6)
}
}
p
mu
sigma
for(i in 1:40){
# E-step
for (k in 1:K){
d[, k] <- pi[k]*dmvnorm(X, mu[k, ], sigma[, , k])
}
total.d <- rowSums(d)
for (k in 1:K){
p[, k] <- pi[k]*dmvnorm(X, mu[k, ], sigma[, , k])/total.d
}
# M-step
pi <- colSums(p)/N
sum.p <- colSums(p)
for (k in 1:K){
mu[k, ] <- colSums(p[, k]*X)/sum.p[k]
}
for (k in 1:K){
temp <- 0
for(j in 1:N){
temp <- temp + p[j, k]*(X[j, ] - mu[k, ])%*%t(X[j, ] - mu[k, ])
}
sigma[, , k] <- temp/sum.p[k]
}
}
sigma
for(i in 1:40){
# E-step
for (k in 1:K){
d[, k] <- pi[k]*dmvnorm(X, mu[k, ], sigma[, , k])
}
total.d <- rowSums(d)
for (k in 1:K){
p[, k] <- pi[k]*dmvnorm(X, mu[k, ], sigma[, , k])/total.d
}
# M-step
pi <- colSums(p)/N
sum.p <- colSums(p)
for (k in 1:K){
mu[k, ] <- colSums(p[, k]*X)/sum.p[k]
}
for (k in 1:K){
temp <- 0
for(j in 1:N){
temp <- temp + p[j, k]*(X[j, ] - mu[k, ])%*%t(X[j, ] - mu[k, ])
}
sigma[, , k] <- temp/sum.p[k] + 0.01 * diag((6))
}
}
p
sigma
library(ggplot2)
library(mvtnorm)
library(bayesm)
# Generating some random data from 5 normal distributions
n <- seq(60, 100, 10)
library(mvtnorm)
library(bayesm)
set.seed(5)
X <- rmvnorm(n[1], mean=rnorm(2, 0, 5), sigma=rwishart(3, diag(2))$IW)
for(i in 2:5){
X <- rbind(X, rmvnorm(n[i], mean=rnorm(2, 0, 5), sigma=rwishart(5, diag(2))$IW))
}
N <- nrow(X)
data <- data.frame(x1 = X[, 1], x2 = X[, 2])
library(bayesm)
p <- ggplot(data, aes(x1, x2))
p + geom_point()
library(mclust)
seeds <- read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data",
header=F)
x <- cbind(seeds[7],seeds[17],seeds[18],seeds[34], seeds[39],seeds[128])
X = data.matrix(x)
BIC = mclustBIC(X)
mod1 = Mclust(X, x = BIC)
summary(mod1, parameters = TRUE)
View(seeds)
View(seeds)
View(seeds)
seeds <- read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data",
header=F)
x <- cbind(seeds[7],seeds[17],seeds[18],seeds[34], seeds[39],seeds[37],seeds[128])
X = data.matrix(x)
BIC = mclustBIC(X)
plot(BIC)
summary(BIC)
mod1 = Mclust(X, x = BIC)
summary(mod1, parameters = TRUE)
mod1 = Mclust(X, G=4, x = BIC)
summary(mod1, parameters = TRUE)
seeds <- read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data",
header=F)
x <- cbind(seeds[7],seeds[17],seeds[18],seeds[34], seeds[39],seeds[37],seeds[128])
X = data.matrix(x)
summary(X)
N <- nrow(X)
K = 4
pi <- rep(1/K, K)
sigma <- array(diag(7), c(7, 7, K))
mu <- cbind(runif(K, min(X[, 1]), max(X[, 1])), runif(K, min(X[, 2]), max(X[, 2])),
runif(K, min(X[, 3]), max(X[, 3])),runif(K, min(X[, 4]), max(X[, 4])),
runif(K, min(X[, 5]), max(X[, 5])),runif(K, min(X[, 6]), max(X[, 6])),
runif(K, min(X[, 7]), max(X[, 7])))
d <- matrix(NA, N, K)
p <- matrix(NA, N, K)
for(i in 1:20){
# E-step
for (k in 1:K){
d[, k] <- pi[k]*dmvnorm(X, mu[k, ], sigma[, , k])
}
total.d <- rowSums(d)
for (k in 1:K){
p[, k] <- pi[k]*dmvnorm(X, mu[k, ], sigma[, , k])/total.d
}
# M-step
pi <- colSums(p)/N
sum.p <- colSums(p)
for (k in 1:K){
mu[k, ] <- colSums(p[, k]*X)/sum.p[k]
}
for (k in 1:K){
temp <- 0
for(j in 1:N){
temp <- temp + p[j, k]*(X[j, ] - mu[k, ])%*%t(X[j, ] - mu[k, ])
}
sigma[, , k] <- temp/sum.p[k] + 0.00011 * diag((7))
}
}
sigma
mu
p
clus <- apply(p, 1, which.max)
clus
summary(clus)
mu
a <- table(p)
a
p
a <- table(clus)
a
for(i in 1:100){
# E-step
for (k in 1:K){
d[, k] <- pi[k]*dmvnorm(X, mu[k, ], sigma[, , k])
}
total.d <- rowSums(d)
for (k in 1:K){
p[, k] <- pi[k]*dmvnorm(X, mu[k, ], sigma[, , k])/total.d
}
# M-step
pi <- colSums(p)/N
sum.p <- colSums(p)
for (k in 1:K){
mu[k, ] <- colSums(p[, k]*X)/sum.p[k]
}
for (k in 1:K){
temp <- 0
for(j in 1:N){
temp <- temp + p[j, k]*(X[j, ] - mu[k, ])%*%t(X[j, ] - mu[k, ])
}
sigma[, , k] <- temp/sum.p[k] + 0.00011 * diag((7))
}
}
clus <- apply(p, 1, which.max)
a <- table(clus)
a
mu
sigma
seeds <- read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data",
header=F)
x <- cbind(seeds[7],seeds[17],seeds[18],seeds[34], seeds[39],seeds[37],seeds[128])
N <- nrow(x)
data <- data.frame(x1 = x[1], x2 = x[2],x3=x[3],x4=x[4],x5=x[5],x6=x[6],x7=x[7])
K = 4
Cent <- cbind(runif(K, min(x[, 1]), max(x[, 1])), runif(K, min(x[, 2]), max(x[, 2])))
for(k in 3:7){
Cent <- cbind(Cent, runif(K, min(x[, k]), max(x[, k])))
}
D <- matrix(NA, N, K)
for(i in 1:100){
# Calculating the distance of each data point from the K centroids.
for (k in 1:K){
D[, k] <- rowSums( (sweep(x, 2, Cent[k, ]))^2)
}
# Assigning the data points to the closest centroid
clus <- apply(D, 1, which.min)
# Finding the new centroids
Cent <- by(x, INDICES=clus, FUN=colMeans)
Cent <- do.call(rbind, Cent)
}
toplot <- data.frame(x1 = x[,3], x2 = x[,4], clus = factor(clus) )
p <- ggplot(toplot, aes(x1, x2))
p + geom_point(aes(color=clus, shape=clus))
Cent
clus
table(clus)
kmeans(x, centers = 4)
Cent
abs(Cent - Cent/2)
max(abs(Cent - Cent/2))
for(i in 1:200){
# Calculating the distance of each data point from the K centroids.
for (k in 1:K){
D[, k] <- rowSums( (sweep(x, 2, Cent[k, ]))^2)
}
# Assigning the data points to the closest centroid
clus <- apply(D, 1, which.min)
# Finding the new centroids
Cent_new <- by(x, INDICES=clus, FUN=colMeans)
Cent_new <- do.call(rbind, Cent)
# We can also set a threshold value for updating the Cent
if(max(abs(Cent_new - Cent)) < 1e-6){
break
}
else
Cent = Cent_new
}
for(i in 1:200){
# Calculating the distance of each data point from the K centroids.
for (k in 1:K){
D[, k] <- rowSums( (sweep(x, 2, Cent[k, ]))^2)
}
# Assigning the data points to the closest centroid
clus <- apply(D, 1, which.min)
# Finding the new centroids
Cent_new <- by(x, INDICES=clus, FUN=colMeans)
Cent_new <- do.call(rbind, Cent_new)
# We can also set a threshold value for updating the Cent
if(max(abs(Cent_new - Cent)) < 1e-6){
break
}
else
Cent = Cent_new
}
Cent
View(mu)
View(mu)
View(mu)
View(mu)
mu
write.csv2(mu,file = "mu.csv")
getwd
getwd()
write.csv2(mu,file = "~/Desktop/STAT235/HW/mu.csv")
Cent
clus
clus
table(clus)
a = table(clus)
a
mu
rbind(mu, a)
cbind(mu, a)
x = cbind(mu, a)
x
table(mu)
x
write.csv2(x,file = "~/Desktop/STAT235/HW/x.csv")
mu
write.csv2(mu,file = "~/Desktop/STAT235/HW/mu.csv", quote = F )
library(splines)
set.seed(4)
x = runif(100, -3, 3)
y = (1 + 2*x - 3*sin(x)) + rnorm(100, 0, .5)
plot(x, y)
x = runif(100, -3, 3)
y = (1 + 2*x - 3*sin(x)) + rnorm(100, 0, .5)
plot(x, y)
abline(v=-1, col='black')
abline(v=1, col='black')
h1 = x <=- 1
h2 = x > -1 & x <= 1
h3 = x > 1
H = cbind(h1, h2, h3)
H
beta = solve(t(H)%*%H)%*%t(H)%*%y
beta
y.hat = H%*%beta
lines(c(-3, -1), c(beta[1], beta[1]), lwd=2)
lines(c(-1, 1), c(beta[2], beta[2]), lwd=2)
lines(c(1, 3), c(beta[3], beta[3]), lwd=2)
plot(x, y)
abline(v=-1, col='black')
abline(v=1, col='black')
basis.fn <- bs(x, knots=c(-1, 1), degree=1)
B.model <- lm(y~basis.fn)
beta = coef(B.model)
t <- seq(-3, 3, .1)
basis.fn.t = predict(basis.fn, t)
y.hat <-cbind(1, basis.fn.t)%*%beta
lines(t, y.hat, lwd=2)
plot(x, y, xlim=c(-5, 5), ylim=c(-10, 10))
abline(v=-1, col='black')
abline(v=1, col='black')
basis.fn <- bs(x, knots=c(-1, 1), degree=3)
cubicSp.model <- lm(y~basis.fn)
beta = coef(cubicSp.model)
t <- seq(-5, 5, .1)
basis.fn.t = predict(basis.fn, t)
y.hat <-cbind(1, basis.fn.t)%*%beta
lines(t, y.hat, lwd=2)
basis.fn <- ns(x, knots=c(-1, 1))
beta = coef(lm(y~basis.fn))
t <- seq(-5, 5, .1)
basis.fn.t = predict(basis.fn, t)
y.hat <-cbind(1, basis.fn.t)%*%beta
lines(t, y.hat, lwd=2, lty=2)
# Smoothing splines
plot(x, y)
abline(v=-1, col='black')
abline(v=1, col='black')
ss.model <- smooth.spline(x, y, spar=0.1)
lines(ss.model, lwd=2)
ss.model <- smooth.spline(x, y, spar=0.9)
lines(ss.model, lwd=2)
ss.model <- smooth.spline(x, y, df = 30)
lines(ss.model, lwd=2)
ss.model <- smooth.spline(x, y, df = 10)
lines(ss.model, lty=2, lwd=2)
ss.model <- smooth.spline(x, y, df = 3)
lines(ss.model, lty=3, lwd=2)
legend('topleft', legend=c(expression(paste(df[lambda], '=30')), expression(paste(df[lambda], '=10')), expression(paste(df[lambda], '=3')) ), lty=c(1, 2, 3), lwd=2)
basis.fn <- bs(x, knots=c(-1, 1), degree=1)
basis.fn
plot(x, y, xlim=c(-5, 5), ylim=c(-10, 10))
abline(v=-1, col='black')
abline(v=1, col='black')
basis.fn <- bs(x, knots=c(-1, 1), degree=3)
cubicSp.model <- lm(y~basis.fn)
beta = coef(cubicSp.model)
t <- seq(-5, 5, .1)
basis.fn.t = predict(basis.fn, t)
h1 = x <=- 1
h2 = x > -1 & x <= 1
h3 = x > 1
H = cbind(h1, h2, h3)
H
x
beta = solve(t(H)%*%H)%*%t(H)%*%y
beta
solve(t(H)%*%H)
t(H)%*%H
H
t(H)%*%H
t(H)
solve(t(H)%*%H)
solve(t(H)%*%H)%*%t(H)
solve(t(H)%*%H)%*%t(H)%*%y
H
H%*%beta
lines(c(-3, -1), c(beta[1], beta[1]), lwd=2)
lines(c(-1, 1), c(beta[2], beta[2]), lwd=2)
lines(c(1, 3), c(beta[3], beta[3]), lwd=2)
library(pROC)
library(boot)
library(randomForest)
library(rpart)
library(party)
library(car)
source('BayesianRegARD.R')
library(ggplot2)
setwd
setwd("~/Desktop/STAT235/Code/")
source('BayesianRegARD.R')
library(ggplot2)
x <- matrix(rnorm(50000), 1000, 50)
b0 <- c(0, rnorm(10, 0, 5), rnorm(20, 0, 2), rep(0, 20))
y = cbind(1, x)%*%b0
categories <- c('A', 'B', 'C')
varCats <- c(rep('A', 10), rep('B', 20), rep('C', 20))
vars <- paste('var', seq(1, 50), sep='.')
nVar <- length(vars)
cats <- data.frame(start=NA, end=NA)
cats <- data.frame(start=NA, end=NA)
for(i in 1:length(categories)){
ind = which(varCats==categories[i])
cats[i, 'start'] = min(ind)
cats[i, 'end'] = max(ind)
}
cats$categories <- categories
cats$end <- cats$end+1
# The function BayesianLinReg uses Gibbs sampling to sample from the posterior distribution of model parameters
samp <- BayesianLinReg(x, y, vars, varCats, categories, nIterations=10000)
# Posterior means of regression coefficients
b <- as.vector(rowMeans(samp$beta[2:(nVar+1), 2000:10000]))
beta <- data.frame(x = seq(1, nVar), b = b, cat=varCats)
ggplot(data=beta, aes(x=x, y=b)) + geom_point(cex=3) + ylab(expression(beta)) +
#theme(plot.margin = unit(c(1, 0, 0, 0),"lines")) +
xlab('Variables')+scale_x_discrete(labels=vars) +
theme(axis.text.x = element_text(angle = 90, hjust = 1, size=7))+
geom_rect(aes(NULL, NULL, xmin = start, xmax = end, fill = categories), ymin = min(beta$b)-1, ymax = max(beta$b)+1, alpha=I(1/5), data = cats)+
geom_hline(aes(yintercept=0))
# Posterior means of the three hyperparameters
tau2 <- rowMeans(samp$tau2[, 2000:10000])
tau <- data.frame(x = seq(1, length(categories)), t = as.vector(tau2))
#pdf(file='BayesianTau2.pdf')
ggplot(data=tau, aes(x=x, y=t)) + geom_point(cex=3) + ylab(expression(tau^2))  +
#  theme(plot.margin = unit(c(1, 1, 3, 1),"lines")) +
xlab('Variables')+scale_x_discrete(labels=categories) +
theme(axis.text.x = element_text(angle = 90, hjust = 1, size=9))+
geom_hline(aes(yintercept=0))
library(neuralnet)
install.packages("neuralnet")
library(neuralnet)
set.seed(4)
x = runif(100, -3, 3)
y = (1 + 2*x - 3*sin(x)) + rnorm(100, 0, .5)
plot(x, y)
data <- data.frame(y = y, x = x)
View(data)
nn <- neuralnet(y~x, data=data, hidden = 2, linear.output = TRUE)
plot(nn)
nn <- neuralnet(y~x, data=data, hidden = 5, linear.output = TRUE)
nn <- neuralnet(y~x, data=data, hidden = 2, linear.output = TRUE)
